# GUL v3.2 Compiler - Token Definitions
# Defines all token types used by the lexer

@imp std.collections

# Token types enumeration
enum TokenType:
    # Literals
    Integer
    Float
    String
    Boolean
    
    # Identifiers and Keywords
    Identifier
    
    # Keywords
    Let
    Var
    Fn
    Async
    Struct
    Enum
    Match
    If
    Elif
    Else
    For
    While
    Loop
    In
    Break
    Continue
    Return
    Try
    Catch
    Finally
    
    # Import system
    AtImp          # @imp
    AtPython       # @python
    AtRust         # @rust
    AtSql          # @sql
    AtJs           # @js
    AtUi           # @ui
    
    # Type constructors
    AtInt          # @int
    AtFloat        # @float
    AtStr          # @str
    AtBool         # @bool
    AtList         # @list
    AtTuple        # @tuple
    AtSet          # @set
    AtDict         # @dict
    
    # Ownership modes
    Borrow
    Ref
    Move
    Kept
    
    # Operators
    Plus           # +
    Minus          # -
    Star           # *
    Slash          # /
    Percent        # %
    DoubleStar     # **
    
    # Comparison
    EqualEqual     # ==
    NotEqual       # !=
    Greater        # >
    GreaterEq      # >=
    Less           # <
    LessEq         # <=
    
    # Logical
    And            # &&
    Or             # ||
    Not            # not
    
    # Assignment
    Equal          # =
    PlusEq         # +=
    MinusEq        # -=
    StarEq         # *=
    SlashEq        # /=
    
    # Delimiters
    LeftParen      # (
    RightParen     # )
    LeftBrace      # {
    RightBrace     # }
    LeftBracket    # [
    RightBracket   # ]
    
    # Punctuation
    Comma          # ,
    Colon          # :
    Semicolon      # ;
    Dot            # .
    Arrow          # ->
    FatArrow       # =>
    DotDot         # ..
    DotDotEq       # ..=
    
    # Special
    Newline
    Indent
    Dedent
    Eof
    Comment
    Error
    
    # Entry point
    Mn             # mn:

# Token struct representing a single lexical token
struct Token:
    token_type: TokenType
    value: @str
    line: @int
    column: @int

impl Token:
    fn to_string(ref self) -> @str:
        return format!("{self.token_type}('{self.value}') at {self.line}:{self.column}")
    
    pub fn is_keyword(&self) -> bool:
        """Check if token is a keyword"""
        let keywords = HashSet::from([TokenType::Let, TokenType::Var, TokenType::Fn,
            TokenType::Async, TokenType::Struct, TokenType::Enum,
            TokenType::Match, TokenType::If, TokenType::Elif,
            TokenType::Else, TokenType::For, TokenType::While,
            TokenType::Loop, TokenType::In, TokenType::Break,
            TokenType::Continue, TokenType::Return, TokenType::Try,
            TokenType::Catch, TokenType::Finally, TokenType::Mn]);
        return keywords.contains(&self.token_type)
    
    pub fn is_operator(&self) -> bool:
        """Check if token is an operator"""
        let operators = HashSet::from([TokenType::Plus, TokenType::Minus, TokenType::Star,
            TokenType::Slash, TokenType::Percent, TokenType::DoubleStar,
            TokenType::EqualEqual, TokenType::NotEqual,
            TokenType::Greater, TokenType::GreaterEq,
            TokenType::Less, TokenType::LessEq,
            TokenType::And, TokenType::Or, TokenType::Not]);
        return operators.contains(&self.token_type)
    
    pub fn is_literal(&self) -> bool:
        """Check if token is a literal value"""
        let literals = HashSet::from([TokenType::Integer, TokenType::Float,
            TokenType::String, TokenType::Boolean,
            TokenType::NoneLiteral]);
        return literals.contains(&self.token_type)

# Helper function to create tokens
fn create_token(token_type: TokenType, value: @str, line: @int, column: @int) -> Token:
    let t = Token{
        token_type: token_type,
        value: value,
        line: line,
        column: column,
    };
    return t

fn get_keyword_type(word: @str) -> TokenType:
    """Get the token type for a keyword"""
    if word == "let":
        return TokenType.Let
    elif word == "var":
        return TokenType.Var
    elif word == "fn":
        return TokenType.Fn
    elif word == "async":
        return TokenType.Async
    elif word == "struct":
        return TokenType.Struct
    elif word == "enum":
        return TokenType.Enum
    elif word == "match":
        return TokenType.Match
    elif word == "if":
        return TokenType.If
    elif word == "elif":
        return TokenType.Elif
    elif word == "else":
        return TokenType.Else
    elif word == "for":
        return TokenType.For
    elif word == "while":
        return TokenType.While
    elif word == "loop":
        return TokenType.Loop
    elif word == "in":
        return TokenType.In
    elif word == "break":
        return TokenType.Break
    elif word == "continue":
        return TokenType.Continue
    elif word == "return":
        return TokenType.Return
    elif word == "try":
        return TokenType.Try
    elif word == "catch":
        return TokenType.Catch
    elif word == "finally":
        return TokenType.Finally
    elif word == "mn":
        return TokenType.Mn
    elif word == "borrow":
        return TokenType.Borrow
    elif word == "ref":
        return TokenType.Ref
    elif word == "move":
        return TokenType.Move
    elif word == "kept":
        return TokenType.Kept
    elif word == "and":
        return TokenType.And
    elif word == "or":
        return TokenType.Or
    elif word == "not":
        return TokenType.Not
    elif word == "true":
        return TokenType.Boolean
    elif word == "false":
        return TokenType.Boolean
    return TokenType.Error

fn is_keyword(word: @str) -> @bool:
    """Check if word is a reserved keyword"""
    return get_keyword_type(word) != TokenType.Error

fn get_type_constructor_type(text: @str) -> TokenType:
    """Get the token type for a type constructor"""
    # Check for @ prefix (byte 64)
    if text.len() < 2 or text.as_bytes()[0] != 64:
        return TokenType.Error
    
    let suffix = &text[1..]
    if suffix == "int":
        return TokenType.AtInt
    elif suffix == "float":
        return TokenType.AtFloat
    elif suffix == "str":
        return TokenType.AtStr
    elif suffix == "bool":
        return TokenType.AtBool
    elif suffix == "list":
        return TokenType.AtList
    elif suffix == "tuple":
        return TokenType.AtTuple
    elif suffix == "set":
        return TokenType.AtSet
    elif suffix == "dict":
        return TokenType.AtDict
    return TokenType.Error

fn is_type_constructor(text: @str) -> @bool:
    """Check if text is a type constructor"""
    return get_type_constructor_type(text) != TokenType.Error

fn get_decorator_type(text: @str) -> TokenType:
    """Get the token type for a decorator"""
    # Check for @ prefix (byte 64)
    if text.len() < 2 or text.as_bytes()[0] != 64:
        return TokenType.Error

    let suffix = &text[1..]
    if suffix == "imp":
        return TokenType.AtImp
    elif suffix == "python":
        return TokenType.AtPython
    elif suffix == "rust":
        return TokenType.AtRust
    elif suffix == "sql":
        return TokenType.AtSql
    elif suffix == "js":
        return TokenType.AtJs
    elif suffix == "ui":
        return TokenType.AtUi
    return TokenType.Error

fn is_decorator(text: @str) -> @bool:
    """Check if text is a decorator/annotation"""
    return get_decorator_type(text) != TokenType.Error
